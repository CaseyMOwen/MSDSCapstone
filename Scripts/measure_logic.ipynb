{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "measures_df = pd.read_csv('measures.csv')\n",
    "\n",
    "with open('measure_logic/2022/EUSS-project-file.yml', 'r') as file:\n",
    "    logic = yaml.safe_load(file)\n",
    "\n",
    "for upgrade in logic['upgrades']:\n",
    "    logic['upgrades']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HVAC Heating Efficiency', 'Heating Fuel', 'Vintage', 'HVAC Heating Type And Fuel', 'Lighting', 'Energystar Climate Zone 2023'}\n"
     ]
    }
   ],
   "source": [
    "def get_yaml_objs():\n",
    "    '''\n",
    "    Reads the relevant yaml files that describe what are the requirements for a measure to be considered \"applicable\", and combines all relevant blocks, one for each measure, into a single dict.\n",
    "    '''\n",
    "    measures_df = pd.read_csv('measures.csv')\n",
    "    yaml_dict = {'2022_1': {}, '2024_1': {}}\n",
    "    for i, row in measures_df.iterrows():\n",
    "        # if row['upgrade_name'] not in yaml_dict:\n",
    "        if row['name'] == \"Baseline\":\n",
    "            continue\n",
    "        with open(row['measure_info_file'], 'r') as f:\n",
    "            obj = yaml.safe_load(f)\n",
    "            for upgrade in obj['upgrades']:\n",
    "                if upgrade['upgrade_name'] == row['upgrade_name']:\n",
    "                    yaml_dict[row['resstock_version']][row['measure_id']] = upgrade\n",
    "    return yaml_dict\n",
    "\n",
    "def get_dependent_applicability_cols(yaml_dict, release):\n",
    "    '''\n",
    "    Parses the yaml dict and returns the columns, in capital case, that are referenced to determine applicability. Recursive wrapper for get_statement_cols.\n",
    "    '''\n",
    "    # pass\n",
    "    dep_cols = set()\n",
    "    for measure_id in yaml_dict[release]:\n",
    "        # measure = yaml_dict[measure_id]\n",
    "        # print(yaml_dict[measure_id]['upgrade_name'])\n",
    "        for option in yaml_dict[release][measure_id]['options']:\n",
    "            if 'apply_logic' not in option:\n",
    "                continue\n",
    "            option_ele = option['apply_logic']\n",
    "            # Sometimes this is formatted where the statement is the only object in a list\n",
    "            if type(option_ele) is list:\n",
    "                option_ele = option_ele[0]\n",
    "            # print(option_ele)\n",
    "            statement_cols = get_statement_cols(option_ele)\n",
    "            # print(statement_cols)\n",
    "            dep_cols = set.union(dep_cols, statement_cols)\n",
    "    return dep_cols\n",
    "\n",
    "def get_statement_cols(statement):\n",
    "    '''\n",
    "    Recursively determines what columns are required to determine the statements truth. Recursively gets the union of the set dependent columns of all nested statements. \n",
    "    '''\n",
    "    if type(statement) is list and len(statement) == 1:\n",
    "        return statement[0]\n",
    "    if type(statement) is str:\n",
    "        # Recursion base case - parse from seperator\n",
    "        column = statement.split('|')[0]\n",
    "        return {column}\n",
    "    # Statment is a dict with either key 'and' or key 'or'\n",
    "    elif 'and' in statement:\n",
    "        return set.union(*[get_statement_cols(item) for item in statement['and']])\n",
    "    elif 'or' in statement:\n",
    "        # output = [get_statement_cols(item) for item in statement['or']]\n",
    "        # print(f'or output: {output}')\n",
    "        return set.union(*[get_statement_cols(item) for item in statement['or']])\n",
    "    elif 'not' in statement:\n",
    "        # output = get_statement_cols(statement['not'])\n",
    "        # print(f'not output: {output}')\n",
    "        return get_statement_cols(statement['not'])\n",
    "\n",
    "# yaml_dict = get_yaml_objs()\n",
    "# dep_cols = get_dependent_applicability_cols(yaml_dict, '2024_1')\n",
    "# print(dep_cols)\n",
    "# print(get_statement_cols({'and': [{'or': ['ASHRAE IECC Climate Zone 2004|4A', 'ASHRAE IECC Climate Zone 2004|4B', 'ASHRAE IECC Climate Zone 2004|4C', 'ASHRAE IECC Climate Zone 2004|5A', 'ASHRAE IECC Climate Zone 2004|5B', 'ASHRAE IECC Climate Zone 2004|6A', 'ASHRAE IECC Climate Zone 2004|6B', 'ASHRAE IECC Climate Zone 2004|7A', 'ASHRAE IECC Climate Zone 2004|7B']}, 'Geometry Attic Type|Vented Attic', {'or': ['Insulation Ceiling|Uninsulated', 'Insulation Ceiling|R-7', 'Insulation Ceiling|R-13', 'Insulation Ceiling|R-19', 'Insulation Ceiling|R-30', 'Insulation Ceiling|R-38']}]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def to_underscore_case(s):\n",
    "    # Replace '::' with '/'\n",
    "    s = s.replace('::', '/')\n",
    "    \n",
    "    # Convert CamelCase to snake_case\n",
    "    s = re.sub(r'([a-z\\d])([A-Z])', r'\\1_\\2', s)\n",
    "    s = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\\1_\\2', s)\n",
    "    \n",
    "    # Replace '-' and ' ' with '_'\n",
    "    s = s.replace('-', '_').replace(' ', '_')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    return \"in.\" + s\n",
    "\n",
    "# Feats is dictionary with unserscore case field names, and value as value\n",
    "\n",
    "def lookup_county_puma(feats:dict):\n",
    "    '''\n",
    "    Replaces geoid with county and puma in feats dict\n",
    "    '''\n",
    "    if not \"geoid\" in feats:\n",
    "        raise ValueError(\"feats must include at least geoid as a key\")\n",
    "    \n",
    "    with open('../API/geoid_lookup.json') as json_file:\n",
    "        geoid_lookup = json.load(json_file)\n",
    "\n",
    "    geoid = feats.pop('geoid')\n",
    "    county_and_puma = geoid_lookup[geoid]\n",
    "    # del feats[\"geoid\"]\n",
    "    feats[\"in.county_and_puma\"] = county_and_puma\n",
    "    return feats\n",
    "\n",
    "def generate_sample(feats:dict, num_samples:int, release:str, yaml_dict):\n",
    "# For each row in dependencies list - check if we have what we need for this column, and if we need it. If so, add a column to the samples df, each of correct distribution according to the existing row\n",
    "    dep_df = pd.read_csv('../API/appfiles/schema/' + release + '_dependencies.csv')\n",
    "    column_plan_df = pd.read_csv('../API/column_plan.csv', usecols=['field_name','keep_for_model'])\n",
    "\n",
    "    # List of columns that the model requires as inputs\n",
    "    needed_model_und_cols = column_plan_df.loc[\n",
    "        (column_plan_df['keep_for_model'] == 'Yes') | \n",
    "        (column_plan_df['keep_for_model'] == 'Split')\n",
    "    ]['field_name'].to_list()\n",
    "    \n",
    "    # Set of cols needed to calculate measure applicability \n",
    "    needed_applic_cap_cols = get_dependent_applicability_cols(yaml_dict, release)\n",
    "    needed_applic_und_cols = [to_underscore_case(item) for item in needed_applic_cap_cols]\n",
    "\n",
    "    needed_und_cols = list(set.union(set(needed_model_und_cols), set(needed_applic_und_cols)))\n",
    "\n",
    "    if release == \"2022_1\":\n",
    "        needed_und_cols.remove('in.duct_location')\n",
    "        needed_und_cols.remove('in.household_has_tribal_persons')\n",
    "        needed_und_cols.remove('in.clothes_washer_usage_level')\n",
    "        needed_und_cols.remove('in.clothes_dryer_usage_level')\n",
    "        needed_und_cols.remove('in.cooking_range_usage_level')\n",
    "        needed_und_cols.remove('in.refrigerator_usage_level')\n",
    "        needed_und_cols.remove('in.duct_leakage_and_insulation')\n",
    "        if 'in.ducts' not in needed_und_cols:\n",
    "            needed_und_cols.append('in.ducts')\n",
    "\n",
    "    needed_und_cols.append(\"in.county_and_puma\")\n",
    "    # Initialize with known parameters\n",
    "    sample_df_dict = {'bldg_id': list(range(num_samples))}\n",
    "    for key in feats:\n",
    "        sample_df_dict[key] = [feats[key]]*num_samples\n",
    "        # print(f'removing key: {key}')\n",
    "        needed_und_cols.remove(key)\n",
    "    \n",
    "    sample_df = pl.DataFrame(sample_df_dict)\n",
    "    known_cap_cols  = set(dep_df[dep_df['UnderscoreCase'].isin(feats.keys())]['CapitalCase'].to_list())\n",
    "    needed_cap_cols = set(dep_df[dep_df['UnderscoreCase'].isin(needed_und_cols)]['CapitalCase'].to_list())\n",
    "    iter = 0\n",
    "    while needed_cap_cols:\n",
    "        iter += 1\n",
    "        # print(f'iter: {iter}, needed_cap_cols: {needed_cap_cols}')\n",
    "        # print(f'iter: {iter}, known_cap_cols: {known_cap_cols}')\n",
    "        for i, field in dep_df['CapitalCase'].items():\n",
    "            if field not in needed_cap_cols or field in known_cap_cols:\n",
    "                continue\n",
    "            dep_str = dep_df.loc[i, 'Dependencies']\n",
    "            # print(dep_str)\n",
    "            if pd.isnull(dep_str):\n",
    "                dependencies = []\n",
    "            else:\n",
    "                dependencies = dep_str.split('|')\n",
    "            \n",
    "            if any(x not in known_cap_cols for x in dependencies):\n",
    "                # Set difference - only add dependencies not already known\n",
    "                needed_cap_cols.update(set(dependencies) - known_cap_cols)\n",
    "                continue\n",
    "            else: #All dependencies are already in sample_df\n",
    "                sample_df = add_col_to_sample(sample_df, field, dependencies, num_samples, release)\n",
    "                needed_cap_cols.remove(field)\n",
    "                known_cap_cols.add(field)\n",
    "        if iter > 500:\n",
    "            break\n",
    "    # Can safely remove all columns not needed for model but for dependencies\n",
    "    model_cols = needed_und_cols + list(feats)\n",
    "    model_cols.remove(\"in.county_and_puma\")\n",
    "    state = sample_df.item(0, \"in.state\")\n",
    "    gisjoin = sample_df.item(0, \"in.county_and_puma\").split(\", \")[0]\n",
    "    return clean_sample(sample_df, model_cols), state, gisjoin\n",
    "    # clean_sample(sample_df, model_cols).write_csv('sample_test.csv')\n",
    "\n",
    "def clean_sample(sample_df: pl.DataFrame, model_cols:list[str]) -> pl.DataFrame:\n",
    "    cleaned_df = (sample_df\n",
    "        .lazy()\n",
    "        # In.state is in.county before comma\n",
    "        .drop(\"in.county\")\n",
    "        .with_columns(\n",
    "            pl.col(\"in.county_and_puma\").str.split(by=\", \").list.first().alias(\"in.county\")\n",
    "        )\n",
    "        .select(model_cols)\n",
    "    )\n",
    "    \n",
    "    return cleaned_df.collect()\n",
    "\n",
    "\n",
    "def add_col_to_sample(sample_df: pl.DataFrame, cap_field: str, cap_dependencies: list[str], num_points: int, release:str=\"2024_1\"):\n",
    "    directory = '../API/appfiles/housing_characteristics/' + release\n",
    "    char_df = pl.scan_parquet(directory + '/' + cap_field + '.parquet')\n",
    "    options = [col.split('Option=')[1] for col in char_df.columns if col.startswith('Option=')]\n",
    "\n",
    "    # Easy case where there are no dependencies - only top row of char_df matters\n",
    "    if not cap_dependencies:\n",
    "        probs = (char_df\n",
    "            .lazy()\n",
    "            .select(pl.selectors.starts_with('Option='))\n",
    "            .cast(pl.Float64)\n",
    "            .collect()\n",
    "            .to_numpy()\n",
    "        )[0]\n",
    "        probs /= np.sum(probs)\n",
    "        samples = np.random.choice(options, p=probs, size=num_points,replace=True).tolist()\n",
    "        sample_df = sample_df.with_columns(pl.Series(samples).alias(to_underscore_case(cap_field)))\n",
    "        return sample_df\n",
    "    \n",
    "    und_dependencies = [to_underscore_case(d) for d in cap_dependencies]\n",
    "    \n",
    "    # Use later to join on\n",
    "    sample_df_with_deps = (sample_df\n",
    "        .lazy()\n",
    "        .with_columns(deps_str=pl.concat_str(und_dependencies, separator=\"|\"))\n",
    "    )\n",
    "\n",
    "    sample_col = (sample_df\n",
    "        .lazy()\n",
    "        # Will join back up with rest of sample later - only care about what impacts the new column for now\n",
    "        .select(und_dependencies)\n",
    "        # Do join on the first dependency (there is guaranteed to be at least one)\n",
    "        .join(char_df, how='left', left_on=to_underscore_case(cap_dependencies[0]), right_on='Dependency=' + cap_dependencies[0], coalesce=False)\n",
    "        # Further filter on all remaining dependencies - ideally would have joined on all dependencies but not implemented in polars. Equivalent to cross product and filter by multiple columns.\n",
    "        .filter(\n",
    "            # Pl.col(d) is the sample_df column, \"Dependency=\" is the char_df column\" - all dependencies must match exactly\n",
    "            pl.all_horizontal(\n",
    "                pl.col(to_underscore_case(d)) == pl.col('Dependency=' + d) for d in cap_dependencies\n",
    "            )\n",
    "        )\n",
    "        .cast({pl.selectors.starts_with('Option='): pl.Float64})\n",
    "        .with_columns(probs_list=pl.concat_list(pl.selectors.starts_with('Option=')))\n",
    "        # Normalize probabilities to sum to 1\n",
    "        .with_columns(pl.col('probs_list').list.eval(pl.element() / pl.element().sum()))\n",
    "        # Needs to be string rather than list becuase will be joining on it\n",
    "        .with_columns(deps_str=pl.concat_str(pl.selectors.starts_with('Dependency='), separator=\"|\"))\n",
    "        # Grouping before calling numpy.random.choice to take advantage of vectorized version of the function - only calling it the minimum number of times, once per unique combo of dependencies\n",
    "        .group_by(\"deps_str\", \"probs_list\").len(name='count')\n",
    "        .with_columns(\n",
    "            (\n",
    "                # Ideally this would be map batches for peformance but could not get it to work\n",
    "                pl.struct(['probs_list', 'count']).map_elements(\n",
    "                    lambda x: list(np.random.choice(options, p=np.array(x['probs_list'], dtype=float), size=x['count'],replace=True))\n",
    "                    ,return_dtype=pl.List(pl.String)\n",
    "                )\n",
    "            ).alias('choices')\n",
    "        )\n",
    "        # Join back up with unique samples df version where dependencies match, so we now have a list of choices at each sample(row)\n",
    "        .join(sample_df_with_deps, how='inner', on=\"deps_str\", suffix=\"_sample\", coalesce=True)\n",
    "        # Create an index list column where there are several runs of 0 to number of occurences of that dep combo, restarting for each group. Goal is to choose one of every option generated\n",
    "        .with_columns(options_idx=pl.int_range(pl.len()).over(\"deps_str\"))\n",
    "        .with_columns(sample=pl.col(\"choices\").list.get(pl.col(\"options_idx\")))\n",
    "        .rename({'sample': to_underscore_case(cap_field)})\n",
    "        .drop('probs_list', 'deps_str', 'count', 'choices', 'options_idx')\n",
    "    )\n",
    "\n",
    "    return sample_col.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "feats = {\"geoid\": \"0900306\"}\n",
    "feats = lookup_county_puma(feats)\n",
    "yaml_dict = get_yaml_objs()\n",
    "sample_df_2022, state, gisjoin = generate_sample(feats, 100, \"2022_1\", yaml_dict)\n",
    "sample_df_2024, state, gisjoin = generate_sample(feats, 100, \"2024_1\", yaml_dict)\n",
    "# print(sample_df)\n",
    "# print(yaml_objs[1])\n",
    "\n",
    "def get_truth(sample_df: pl.DataFrame, statement) -> pl.DataFrame:\n",
    "    '''\n",
    "    Recursively gets the truth vector for a given statement. Calls either and, or, or not on each nested statement.\n",
    "    '''\n",
    "    if type(statement) is str:\n",
    "        # Recursion base case - parse from seperator\n",
    "        feature, value = statement.split('|')\n",
    "        # print(feature)\n",
    "        # print(value)\n",
    "        # column = to_underscore_case(feature)\n",
    "        # print(column)\n",
    "        return sample_df.select(new = to_underscore_case(feature)).rename({'new': statement}) == value\n",
    "    # Statment is a dict with either key 'and' or key 'or'\n",
    "    elif 'and' in statement:\n",
    "        bool_list = [get_truth(sample_df, item) for item in statement['and']]\n",
    "        df = pl.concat(bool_list, how='horizontal')\n",
    "        new_name = 'and_[' + '|'.join(df.columns) + ']'\n",
    "        df = df.with_columns(pl.all_horizontal(pl.all()).alias(new_name))\n",
    "        # print(df)\n",
    "        return df.select(new_name)\n",
    "        # return all([get_truth(sample_df, item) for item in statement['and']])\n",
    "    elif 'or' in statement:\n",
    "        bool_list = [get_truth(sample_df, item) for item in statement['or']]\n",
    "        df = pl.concat(bool_list, how='horizontal')\n",
    "        new_name = 'or_[' + '|'.join(df.columns) + ']'\n",
    "        df = df.with_columns(pl.any_horizontal(pl.all()).alias(new_name))\n",
    "        # print(df)\n",
    "        return df.select(new_name)\n",
    "        # return any([get_truth(sample_df, item) for item in statement['or']])\n",
    "    elif 'not' in statement:\n",
    "        bool_list = [get_truth(sample_df,statement['not'])]\n",
    "        # print(bool_list)\n",
    "        df = pl.concat(bool_list, how='horizontal')\n",
    "        col_name = df.columns[0]\n",
    "        df = df.select(pl.col(col_name).not_()).rename({col_name: 'not_[' + col_name + ']'})\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_applicability(sample_df, yaml_dict:dict, measure_id, release:str):\n",
    "    '''\n",
    "    Gets the applicability of a given measure as a vector, relative to the sample that was previously generated. Considers the measure applicable if any of the sub-options are applicable to the sample row. Wrapper for recursive get_truth().\n",
    "    '''\n",
    "    yaml_obj = yaml_dict[release][measure_id]\n",
    "    option_applic_vecs = []\n",
    "    option_num = 0\n",
    "    for option in yaml_obj['options']:\n",
    "        if 'apply_logic' not in option:\n",
    "            continue\n",
    "        option_num += 1\n",
    "        option_ele = option['apply_logic']\n",
    "        # Sometimes this is formatted where the statement is the only object in a list\n",
    "        if type(option_ele) is list:\n",
    "            option_ele = option_ele[0]\n",
    "        # print(option_ele)\n",
    "        option_applic_vec = get_truth(sample_df, option_ele)\n",
    "        option_applic_vec = option_applic_vec.rename({option_applic_vec.columns[0]: 'option' + str(option_num)})\n",
    "        # print(option_applic_vec)\n",
    "        option_applic_vecs.append(option_applic_vec)\n",
    "    \n",
    "    options_concat = pl.concat(option_applic_vecs, how='horizontal')\n",
    "    new_col_name = 'measure_' + str(measure_id) + '_applies'\n",
    "    any_option_applies = options_concat.with_columns(pl.any_horizontal(pl.all()).alias(new_col_name)).select(new_col_name)\n",
    "    # print(any_option_applies)\n",
    "    return any_option_applies\n",
    "    # return applic_vec.rename({applic_vec.columns[0]: 'measure_' + str(measure_id) + '_applies'})\n",
    "\n",
    "def add_applic_matrices(sample_df_2022, sample_df_2024, yaml_dict):\n",
    "    '''\n",
    "    Gets applicability vectors for all measures, and horizontally concatenates them with the relevant sample df. Returns the combined dataframes\n",
    "    '''\n",
    "    applic_vecs_2022, applic_vecs_2024 = [], []\n",
    "    measures_df = pd.read_csv('measures.csv')\n",
    "    for i, row in measures_df.iterrows():\n",
    "        if row['name'] == \"Baseline\":\n",
    "            continue\n",
    "        print(row['measure_id'])\n",
    "        release = row['resstock_version']\n",
    "        if release == \"2022_1\":\n",
    "            # sample_df = sample_df_2022\n",
    "            applic_vecs_2022.append(get_applicability(sample_df_2022, yaml_dict, row['measure_id'], release))\n",
    "        elif release == \"2024_1\":\n",
    "            applic_vecs_2024.append(get_applicability(sample_df_2024, yaml_dict, row['measure_id'], release))\n",
    "    \n",
    "    full_sample_2022 = pl.concat([sample_df_2022, pl.concat(applic_vecs_2022, how='horizontal')], how='horizontal')\n",
    "    full_sample_2024 = pl.concat([sample_df_2024, pl.concat(applic_vecs_2024, how='horizontal')], how='horizontal')\n",
    "    return full_sample_2022, full_sample_2024\n",
    "\n",
    "# print(yaml_dict['2024_1'][9]['options'])\n",
    "full_sample_2022, full_sample_2024 = add_applic_matrices(sample_df_2022, sample_df_2024, yaml_dict)\n",
    "# full_sample_2022 = pl.concat([sample_df_2022, matrix],how='horizontal')\n",
    "# full_sample_2024 = pl.concat([sample_df_2024, matrix],how='horizontal')\n",
    "full_sample_2022.write_csv('sample_test_2022.csv')\n",
    "full_sample_2024.write_csv('sample_test_2024.csv')\n",
    "\n",
    "\n",
    "# print(applic_vec)\n",
    "# print(sample_df)\n",
    "# truth_vec = get_truth(sample_df, {'and': ['Insulation Wall|Wood Stud, Uninsulated']})\n",
    "# truth_vec_not = get_truth(sample_df, {'not': 'Insulation Wall|Wood Stud, Uninsulated'})\n",
    "\n",
    "# truth_vec = get_truth(sample_df, {'or': [r'Ducts|30% Leakage, R-4', r'Ducts|30% Leakage, R-6']})\n",
    "# truth_vec = get_truth(sample_df, {'and': [{'or': ['ASHRAE IECC Climate Zone 2004|4A', 'ASHRAE IECC Climate Zone 2004|4B', 'ASHRAE IECC Climate Zone 2004|4C', 'ASHRAE IECC Climate Zone 2004|5A', 'ASHRAE IECC Climate Zone 2004|5B', 'ASHRAE IECC Climate Zone 2004|6A', 'ASHRAE IECC Climate Zone 2004|6B', 'ASHRAE IECC Climate Zone 2004|7A', 'ASHRAE IECC Climate Zone 2004|7B']}, 'Geometry Attic Type|Vented Attic', {'or': ['Insulation Ceiling|Uninsulated', 'Insulation Ceiling|R-7', 'Insulation Ceiling|R-13', 'Insulation Ceiling|R-19', 'Insulation Ceiling|R-30', 'Insulation Ceiling|R-38']}]})\n",
    "# print(truth_vec)\n",
    "# print(truth_vec_not)\n",
    "# print(truth_vec.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_applicability_matrix(sample_df):\n",
    "    '''\n",
    "    Gets a matrix of whether the measure applies to each home. The column represents a given measure applicability, of 1 or 0, and done for each row of generated sample\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def get_applicability_vec(sample_df, measure_name):\n",
    "    \n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
